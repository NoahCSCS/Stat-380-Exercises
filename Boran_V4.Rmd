---
title: "Final Exam"
author: "Boran Sheu & Noah Shimizu"
date: '2022-08-05'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(dplyr)
library(ggplot2)
```

**GitHub Link to RMD File**: 

## Probability Practice

Part A:
Rule of Total Probability: P(A)=P(Y1)*P(Q1|Y1)+P(Y2)*P(Q2|Y2)

Let P(Y) as 65%, P(Q1|Y1)=0.5(Random clickers would click either one with equal probability) and P(Y1)=0.3
0.65=0.3*0.5+0.7*P(Q2|Y2)
P(Q2|Y2)=5/7

Par B: Let's use the Bayes Rule
We are finding P(D|p)=P(D,p)/P(p)

What is the possibility that someone tests positive-->
0.000025*0.993+(1-0.000025)*0.0001
=0.0001248225=P(p)
0.000025=P(D)
What is the possibility that someone tests positive and has the disease-->
P(D,p)=P(D)*P(p)=
0.000025*0.0001248225=3.120563e-09


P(D|p)=P(D,p)/P(p)=
3.120563e-09/0.0001248225=
2.5e-05 

\newpage
## Wrangling the Billboard Top 100

```{r Wrangling, include = FALSE}

df_billboard = read.csv("data/billboard.csv")
```

### A
```{r Wrangling A, echo = FALSE}
df_a = df_billboard %>% group_by(song, performer)
df_a = df_a %>% dplyr::summarise( count = sum (weeks_on_chart))
df_a = df_a[order(-df_a$count),]
df_a[1:20,]
```
### B
```{r wrangling B, echo = FALSE}
df_b = df_billboard %>%
  group_by(year) %>%
  dplyr::summarize(diversity = n_distinct(song))
show(ggplot(data = df_b[3:nrow(df_b)-1,], mapping = aes(x = year, y = diversity)) + geom_point() + labs(y = "Number of Songs", title = "Number of Unique Songs in Billboard Top 100 by Year"))

```
We wanted to determine if musical diversity (ie. the number of unique songs in a given year) has changed with time. We have thus plotted the number of unique songs for each year. We see that the number of unique songs in 1959 is about 640. It then generally increases to 800 at around 1967, only to then begin decreasing drastically, till around 2001, where it bottoms out below 400. Finally, starting at around 2003, it increases rapidly, until the most recent year of 2020, where it is again almost at 800. 

### C
```{r, echo = FALSE}
df_ten_week = df_billboard[df_billboard$weeks_on_chart >= 10,]
df_ten_week = df_ten_week %>% subset(select =c("song", "performer"), axis = 1)
df_ten_week = unique(df_ten_week)
df_ten_week = table(df_ten_week$performer)
df_ten_week = df_ten_week[df_ten_week >= 30]
barplot(sort(df_ten_week), main = "Barplot of Artists with 30+ ten week hits", las = 2)
```

Well, Elton John has obviously the highest number of songs that have ten weeks hit. And total of 19 singers have 30 or more ten weeks hit songs. Except Elton John, Tim McGraw, Michael Jackson and Madonna are also among the top tier of the list. Quite interesting is that, I never heard of Tim McGraw in Taiwan!!

\newpage
## Visual Story Telling Part 1: Green Buildings

```{r read data, echo = FALSE, include = FALSE}
df_green = read.csv("data/greenbuildings.csv")
ggplot(data = df_green) +
  #geom_point(mapping = aes(x = leasing_rate, y = age))
  geom_point(aes(x = stories, y = leasing_rate), alpha = 0.25)
ggplot(data = df_green) + geom_histogram(aes(x = leasing_rate))
ggplot(data = df_green, aes(group = Energystar, y = leasing_rate)) +
  #geom_point(mapping = aes(x = leasing_rate, y = age))
  geom_boxplot( alpha = 0.5)
df_green$rent_resid = df_green$Rent - df_green$cluster_rent
ggplot(data = df_green) + 
  #geom_point(mapping = aes(x = leasing_rate, y = age))
  geom_point(aes(y = size, x = leasing_rate, fill = green_rating), alpha = 0.5) + geom_hline(yintercept = 250000)
```
We first begin with dropping the low occupancy building. While the below histogram does agree that there appears to be an outlying cluster of low occupancy buildings. These are likely un-representative of our new building, as it is being built in Austin, where housing is lacking. However, we choose not to omit them, as we prefer to have as much data in our analysis as possible, and we cannot be sure that they will hurt our analysis. We can model only on the high occupancy buildings, and see if that performs the accuracy of our model, yet this again assumes that we know ahead of time that our building won't be one of these low occupancy buildings, which we cannot be 100\% certain that it will be.
```{r echo = FALSE}
df_green = read.csv("data/greenbuildings.csv")
df_green$was_dropped = df_green$leasing_rate <= 10
ggplot(data = df_green) + geom_histogram(aes(x = leasing_rate, fill = was_dropped))
```

Next, we move on to the stats guru's rent predictions. He first assumes that there are no confounding variables between the interaction between rent and green status. For instance, since one of the main advantages of green buildings is lower energy usage, we might expect that green buildings are more enticing to builders in high energy cost areas. We see this in the boxplot below, where green rating buildings have on median a greater electricity cost then non-green-rated buildings. High electricty cost buildings also tend to have greater rents however, as seen in the dotplot below, perhaps because these costs are passed down to consumers, or just that prices tend to be higher in higher energy cost areas, due to general wealth. 
```{r, echo = FALSE}
df_green = df_green %>% mutate(newbin = cut(total_dd_07, breaks = 8))
#print(ggplot(data = df_green, aes(x = newbin, y = Rent, fill = green_rating) )+
#         geom_boxplot())

ggplot(data = df_green, aes(x = Electricity_Costs, y = log(Rent)) )+
         geom_point()
```
```{r, echo = FALSE}
print(ggplot(data = df_green, aes(group = green_rating, y = Electricity_Costs) )+
         geom_boxplot())
```

We could try and take this variable into account in a more advance regression model. We could also account for facts about the area by predicting rent-cluster_rent, or in other words how much higher rent green buildings are than non-green_buildings in each given area.

Another important fact unaccounted for by the model is that some buildings charge their individuals for utilities, while others do not. Charging for utilities separate from rent is called a net contract. We see in the below boxplot that green_rating and net contracts have some interaction, (ie. In buildings where utilties are charged for, rent is on median the same, regardless of whether the building is green rated. However in non net contract apartments, green rating buidlings come at a premium). We should use this to note that perhaps green rated buildings are more profitable when they are not on a net contract, and we can perhaps expect a higher premium if we place users on a net contract.
```{r, echo = FALSE}
df_green$net = factor(df_green$net)
ggplot(data = df_green, aes(x = net, y = log(Rent), fill = factor(green_rating) ))+
         geom_boxplot()
```


Another area where we could see other variables taken into account is building quality. As we see, green buildings are disproportionately good, and good buildings have higher rents. This variable however is difficult to fully take into account, as a building being green contributes to the quality of the building.
```{r, echo = FALSE}
df_green$quality = 2*df_green$class_a + df_green$class_b
df_green$quality = sapply(df_green$quality, function(x){return(rawToChar(as.raw(67-x)))})
print(table(df_green$quality, df_green$green_rating))
print(ggplot(data = df_green, aes(x = quality, y = Rent)) + geom_boxplot())
```

One way we can try to account for these things is rather than looking at how much rent 

We also note that the stats wizard makes that rent and occupancy rate are not related. If rents are too high or low, it might lead to changes in occupancy rate. It would be perhaps better if we predicted revenue, ie. rent * occupancy * square footage, rather than just rent, and asusming occupancy is fixed.

Overall, it is difficult to fix many of these issues without running advanced statistical models. However, one quick fix is assuming that non-green factors a buildings is a function of proximity, ie. if one thing is affecting a building's rent, then it is likely effecting the rent of nearby buildings as well. Thus, our modeling for how much green buildings may charge in extra rent is how much more rent it charges then the average rent of those building's in it's cluster. Note that each cluster contains 1 green certified building, and many non-green certified building, meaning that we need not worry about our clustering placing all green buildings together, and thus not detecting the green-building premium. The results of this clustering is seen below.

```{r, echo=FALSE}
df_green$green_rating = factor(df_green$green_rating)

ggplot(data = df_green, aes(x = green_rating, y = Rent - cluster_rent, ))+
         geom_boxplot()
rents = df_green %>% group_by(green_rating) %>%
  summarise_at(vars(c(Rent, cluster_rent)), mean)
rents$excess_rent = rents$Rent - rents$cluster_rent
print( paste("Excess Rent:", rents$excess_rent[2]-rents$excess_rent[1]))
rents
```
Here, we see green rating has an excess rent of 3.1244 in dollars per square root per year, as compared to the default 0.712's excess rent. We thus replace the \$2.6 number with \$2.41. Repeating their arithmatic is left as an excercize to the grader :)

##  Capital Metro Data

```{r p3, echo=FALSE, include=FALSE}

library(MASS)
library(ISLR)
library(leaps)
library(Matrix)
library(foreach)
library(mosaic)
library(tidyverse)
library(ggplot2)
library(tidyr)
library(dplyr)
set.seed(1)
df <- read.csv("data/capmetro_UT.csv",header=TRUE)
attach(df)
```
For each hour in the day, we look at the ridership.

```{r p32, echo=FALSE}
df$Total_ridership = abs(df$boarding) + abs(df$alighting)

plot(df$hour_of_day, df$Total_ridership,main="Total Ridership Over A Day",
     xlab="Hour Of Day", ylab="Ridership(max)", col = "blue")
```
From the plot we can see that the ridership were initially low at the beginning of a day (6am), as the commute time starts, which is around 7am - 10am, the ridership goes up fast. And then it starts to drop after 10 am, which most students have all gone to school. At 4pm, students are off school and the ridership increases till 6pm.

Next, combine the month and day of month.
```{r p33, echo=FALSE}
df2 = transform(df, Combined_date=paste(df$day_of_week, df$month, sep="/"))

# aggregate the number of delay times based on Date.
rider_df = aggregate(df2$Total_ridership ~ df2$day_of_week+df2$month, data = df2, sum)
rider_df
rider_df2 = aggregate(df2$Total_ridership ~ df2$month, data = df2, sum)
rider_df2
combined_df = merge(rider_df, rider_df2, by = 'df2$month', sort = TRUE)
combined_df

rider_df = aggregate(df2$Total_ridership ~ df2$day_of_week, data = df2, sum)
#rider_df
names(rider_df)[1] <- "Days_of_week"
names(rider_df)[2] <- "Total_ridership_week"
options(scipen = 100)
barplot(rider_df$Total_ridership_week~rider_df$Days_of_week, xlab = "Days of Week", ylab = "Total Ridership on Days of Week", col = "blue")
#?barplot
#plot(rider_df$`df2$day_of_week`, rider_df$`df2$Total_ridership`)
rider_df2 = aggregate(df2$Total_ridership ~ df2$month, data = df2, sum)
#rider_df2
names(rider_df2)[1] <- "Month"
names(rider_df2)[2] <- "Total_ridership_month"
barplot(rider_df2$Total_ridership_month~rider_df2$Month, xlab = "Month", ylab = "Total Ridership on Month", col = "blue")
```
From the two bar plots we wouldn't say that there is obvious patterns on the three months we have. We can say that October has the highest total ridership while November has the lowest.But there is clearly a pattern on weekdays. Weekdays have much more riderships than weekends and Fridays have fewer ridership compare to other days. That might be some colleges, like us MSBA program, don't have lectures on Friday.

```{r p34, echo=FALSE}
rider_df3 = aggregate(df2$Total_ridership ~ df2$temperature, data = df2, sum)
#rider_df3
names(rider_df3)[1] <- "Temperature"
names(rider_df3)[2] <- "Total_ridership_temperature"
plot(rider_df3$Total_ridership_temperature~rider_df3$Temperature, xlab = "Temperature", ylab = "Total Ridership on Temperature", col = "blue", main = "Ridership vs Temperature", sub = "A plot showing no clear trend between ridership numbers and temperature")

```
There is no obvious pattern between temperature and total ridership.

\newpage
## Portfolio Modeling

### Portfolio 1: Market


Portfolio 1 is built to act like the market, using etf's that each are meant to track the market.

* **33% SPY** ETF designed to track S&P 500

* **33% QQQ** ETF that tracks NASDAQ-100

* **33% IWM** iShares Russell 2000 ETF, tracks 2000 small cap companies
```{r, echo=FALSE}

library(mosaic)
library(quantmod)
mystocks = c("SPY", "QQQ", "IWM")
myprices = getSymbols(mystocks, from = "2017-08-10")
# A chunk of code for adjusting all stocks
# creates a new object addind 'a' to the end
# For example, WMT becomes WMTa, etc
for(ticker in mystocks) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}
# Combine all the returns in a matrix
all_returns = cbind(	ClCl(SPYa),
								ClCl(QQQa),
								ClCl(IWMa))
#head(all_returns)
all_returns = as.matrix(na.omit(all_returns))
```



We know plot the bootstrap distribution of change in wealth, as well as the VaR of 0.05

```{r, echo=FALSE}
# Sample a random return from the empirical joint distribution
# This simulates a random day
return.today = resample(all_returns, 1, orig.ids=FALSE)
# Update the value of your holdings
# Assumes an equal allocation to each asset
total_wealth = 100000
my_weights = c(1/3,1/3,1/3)
holdings = total_wealth*my_weights
holdings = holdings*(1 + return.today)
set.seed(5)
# Now simulate many different possible scenarios  
initial_wealth = 100000
sim1 = do(1000)*{
	total_wealth = initial_wealth
	holdings = my_weights * total_wealth
	n = 20
	wealthtracker = rep(0, n)
	for(today in 1:n) {
		return.today = resample(all_returns, 1, orig.ids=FALSE)
		holdings = holdings*(1+return.today)
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}
hist(sim1[,n]- initial_wealth, breaks=30, main = "Market Portfolio Earnings")
abline(v = (quantile(sim1[,n], 0.05) - initial_wealth), col = "red")
# Calculate 5% value at risk
quantile(sim1[,n], 0.05) - initial_wealth
```

Here, VAR of 5\% is a loss of 8381\$, or about an 8.4\% negative return.

### Portfolio 2: "High Betas" 

Portfolio 2 takes some of the highest beta etf's to maximize risk and return.

* **25% TQQQ** ETF designed to triple returns of NASDAQ-100

* **25% OIH** Oil ETF

* **25% FAS** Designed for 3x return of RUssell 1000

* **25% LABU** Designed to perform 3x S&P Biotech.

```{r, echo=FALSE}
mystocks = c("TQQQ", "OIH", "FAS", "LABU")
myprices = getSymbols(mystocks, from = "2017-08-10")
# A chunk of code for adjusting all stocks
# creates a new object addind 'a' to the end
# For example, WMT becomes WMTa, etc
for(ticker in mystocks) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}
# Combine all the returns in a matrix
all_returns = cbind(	ClCl(TQQQa),
								ClCl(OIHa),
								ClCl(FASa),
								ClCl(LABUa))
#head(all_returns)
all_returns = as.matrix(na.omit(all_returns))
# Compute the returns from the closing prices
```


```{r, echo=FALSE}
# Sample a random return from the empirical joint distribution
# This simulates a random day
return.today = resample(all_returns, 1, orig.ids=FALSE)
# Update the value of your holdings
# Assumes an equal allocation to each asset
total_wealth = 100000
my_weights = c(0.25,0.25,0.25,0.25)
holdings = total_wealth*my_weights
holdings = holdings*(1 + return.today)
set.seed(5)
# Now simulate many different possible scenarios  
initial_wealth = 100000
sim1 = do(1000)*{
	total_wealth = initial_wealth
	weights = c(0.25,0.25,0.25,0.25)
	holdings = weights * total_wealth
	n = 20
	wealthtracker = rep(0, n)
	for(today in 1:n) {
		return.today = resample(all_returns, 1, orig.ids=FALSE)
		holdings = holdings *(1+return.today)
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}
hist(sim1[,n]- initial_wealth, breaks=30, main = "Risky Portfolio Earnings")
abline(v = (quantile(sim1[,n], 0.05) - initial_wealth), col = "red")
# Calculate 5% value at risk
quantile(sim1[,n], 0.05) - initial_wealth
```

Here, losses are much larger, at negative 24,800

## Portfolio 3: Low Beta ETF

Lastly, we use ver low beta etfs to try and underperform the market at lower risk

* **33% FBND** ETF of many bonds

* **33% GLD** Gold Shares

* **33% ACWV** Global equity market ETF 

```{r, echo=FALSE}
mystocks = c("FBND", "GLD", "ACWV")
myprices = getSymbols(mystocks, from = "2017-08-10")
# A chunk of code for adjusting all stocks
# creates a new object addind 'a' to the end
# For example, WMT becomes WMTa, etc
for(ticker in mystocks) {
  print(ticker)
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}
# Combine all the returns in a matrix
all_returns = cbind(ClCl(FBNDa),
								ClCl(GLDa),
								ClCl(ACWVa))
#head(all_returns)
all_returns = as.matrix(na.omit(all_returns))
# Compute the returns from the closing prices
pairs(all_returns)
```

From the pairs correlation matrix, we see that these Bond ETFs are less correlated than the ETFs in Portfolio 1 but higher than Portfolio 2. 

Then, we simulate the 20-day trading period of this portfolio.

```{r, echo=FALSE}
# Sample a random return from the empirical joint distribution
# This simulates a random day
return.today = resample(all_returns, 1, orig.ids=FALSE)
# Update the value of your holdings
# Assumes an equal allocation to each asset
total_wealth = 100000
my_weights = c((1/3),(1/3),(1/3))
holdings = total_wealth*my_weights
holdings = holdings*(1 + return.today)
set.seed(1)
# Now simulate many different possible scenarios  
initial_wealth = 100000
sim1 = do(1000)*{
	total_wealth = initial_wealth
	weights = c((1/3),(1/3),(1/3))
	holdings = weights * total_wealth
	n = 20
	wealthtracker = rep(0, n)
	for(today in 1:n) {
		return.today = resample(all_returns, 1, orig.ids=FALSE)
		holdings = holdings * (1 + return.today)
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}
hist(sim1[,n]- initial_wealth, breaks=30, main = "Unrisky Portfolio")
abline(v = (quantile(sim1[,n], 0.05) - initial_wealth), col = "red")
# Calculate 5% value at risk
quantile(sim1[,n], 0.05) - initial_wealth
```
Here, we see the smallest loss of -3328.81

In conclusion, we saw what we expected. The low beta etf of bonds and gold had very low VAR, at a 3.23\% loss, the very risky portfolio of high leverage etf's had a much higher VAR of about a 25\% loss, and the in market indices lied somehwere in between at around an 8.4\% loss.

\newpage
## Clustering and PCA

We first attempt hierarchical clustering. To judge how well the hierarchical clustering and how it's picked up on the wine and beer, look at the rmse of taste and the entropy of type of wine in the hierarchical tree at various numbers of splits, as seen below.

```{r 1.slr, echo = FALSE}

wine<-read.csv("data/wine.csv")
color = wine$color
quality = wine$quality
wine_scale = wine %>% subset(select = -c(color, quality)) %>% scale()
wine_distance = dist(wine_scale, method='euclidean')
wine_hier = hclust(wine_distance, method='average')
color_ent = c()
quality_rmse = c()
n = nrow(wine)

Entropy = function(p){
  if(p == 0 | p == 1){
    return(0)
  }
  return(- (p*log2(p) + (1-p)*log2(1-p)))
}
for( i in 2:100){
  cluster = cutree(wine_hier, k=i)
  entropy = 0
  sse = 0
  for( j in 1:i){
    cut_qualities = quality[cluster == j]
    hi = sum((cut_qualities - mean(cut_qualities))^2)
    sse = sse + hi
    
    cut_entropy = color[cluster == j]
    p = sum(cut_entropy == "red") / length(cut_entropy)
    entropy = entropy + length(cut_entropy)*Entropy(p)/n
  }
  color_ent = c(color_ent, entropy)
  quality_rmse = c(quality_rmse, sqrt(sse/(n)))
}
df = data.frame(rmse = quality_rmse, entropy = color_ent, splits = 2:100)

ggplot(data = df, aes(x = splits, y = entropy)) + geom_line()
ggplot(data = df, aes(x = splits, y = rmse)) + geom_line()
nrow(wine)
```
Here, we see that there is a significant drop in entropy at the 22nd split, implying that this split divides a large number of red and white wines. This is unlikely to be simply overfitting, as it is a rather significant single drop, and occurs at the 22nd split in a 6000+ entry list. Alternatively, we see a few gentle declines in rmse for quality, implying it is at various splits detecting differences in quality.

We next attempt PCA. We look at how much variance the principal components each explain.
```{r PCA, echo = FALSE}
pca = prcomp(wine_scale)
barplot(pca$sdev, main = "PCA sd Explained", xlab = "Principles Components", names.arg = 1:length(pca$sdev))
#print(pca$x[,1:4])
#lm(pca$x[1])
```
We see that the first 4 principle components do most in explaining the data, while the other's do much so, and we thus choose to only view the first 4 principal components.
```{r, echo = FALSE}
pca$rotation[,1:4]
```
Here, the first principal components seems to be detecting high sulfur dioxide wines. This would explain why these wines are lower ph, as sulfur dioxide is acidic according to google. This also perhaps explains why these wines are lower in other types of acidity, as they are more acidic due to the sulfur dioxide. 

Principal component 2 appears to show very dense very low acohol content wines.

Principal component 3 seems to see low citric acid high ph wines. Citric acid according to google citric acid is relatively weak, and thus likely explains why it has higher ph than other wines.

Lastly, PC 4 seems to detect low sulphate low ph wines, which is detecting what I just said.

We check how much these PC detect quality and different qualities by seeing how well they can be used as a linear model for quality and a logistic regression for wine taste.

```{r}

summary(lm(quality ~ pca$x[,1] + pca$x[,2] + pca$x[,3] + pca$x[,4]))
logmodel = glm(factor(color) ~ pca$x[,1] + pca$x[,2] + pca$x[,3] + pca$x[,4],
        family = "binomial")
results = logmodel$fitted.values > 0.5
acc = mean(results == (color != "red"))
acc
```
Here, we see that we see that our residual square error is relatively high still at 0.781, with R-square of only 0.1786. However, this is not that much better than our other data. We see that it is very good at accessing color however, at 98.7/% accuracy using only the first 4 pc. 

Ultimately, we believe PCA makes more sense on our data. This is for the primary reason that many of the measures are very highly correlated with one another. The amount of free vs total sulfur dioxide will obviously be related, as one is a subset of the other, and the amount of each acid will affect the ph of the wine. While these issues might hurt clustering, causing some extraneous factors effect creating useless sparsness in clustering, PCA is designed for these exact scenarios in mind.

\newpage
## Market segmentation

Our method will be using kmeans to cluster the data, and observe the results. We first begin by cleaning our data, scaling it and reindexing.

```{r, echo=FALSE}
# clustering
# principal component analysis
df <- read.csv("data/social_marketing.csv")
# TRUE. Thus X column is the ID
row.names(df) <- df$X
df <- df[,-1]
columns <- colnames(df)
df <- data.frame(df)
df <- scale(df)
```
We continue with our data 
### Elbow method

```{r, echo=FALSE}
#Elbow Method for finding the optimal number of clusters
wss <- sapply(1:25, 
              function(k){kmeans(df, k, nstart=5,iter.max = 1000 )$tot.withinss})
plot(1:25, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="k",
     ylab="Total within-clusters sum of squares")

```
The Elbow Plot above indicates a kink at $k=14$. Although perhaps just a random drop, we choose to select it as our choice k, since it is a point at which wthin sum squares does not appear to decrease much more from further itterating.

### Running K-Means with 14 clusters

We now look at the centers of some of the largest clusters.
```{r, echo=FALSE}
# Run 14-means with 215 clusters and 25 starts
clust1 = kmeans(df, 14, nstart=25, iter.max = 1000)
centers = clust1$centers[order(clust1$size),]
centers[1:6,]
```
Our first cluster appears to be overwhelmingly spam, with an overwhelmingly large spam value. Our second cluster has a high dating value, and a somewhat large chatter value, implying it contains users focused around dating. The third cluster has an overwhelming large adult value, implying it is users focused on looking at... "adult" things. Cluster 4 has larg values in tv_film, art, and crafts, implying this is used by users who primarily want to discuss various media they consume and look at cool crafts. The fifth cluster appears to be college students, talking mostly about college and tv_film.The sixth is again similar, again college student users, but ones who care more about music. Both clusters 5 and 6 care some about business, perhaps because they are college students looking for jobs.

The other clusters are not discussed for the purpose of brevity, although they would also be shown in a broader analysis.

\newpage 
## The Reuters corpus

Our goal is to predict author of article based upon the article itself. We believe this to be a useful question, as it could perhaps be used as a tool in detecting anonymously written articles or plagarism.

First, we read in the 50 training articles for each of the 50 different authors. Then set training Corpus.
```{r, echo=FALSE}

library(NLP)
library(tm)
library(caret)
library(tidyverse)
library(slam)
library(class)
library(proxy)
# reader function used in class
readerPlain = function(fname){
				readPlain(elem=list(content=readLines(fname)), 
							id=fname, language='en') }

# expand all file paths in training data
train = Sys.glob('data/ReutersC50/C50test/*')
# initiate empty lists to be used in for loop
trainingArticles = NULL
labels = NULL
# read in all training articles
for (name in train) {
  author = substring(name, first=23) # set author name
  #print(author)
  article = Sys.glob(paste0(name,'/*.txt')) # expand articles for each name
  trainingArticles = append(trainingArticles,article) # append articles to list
  labels = append(labels, rep(author, length(article))) # append labels to list
}

# read all the plain text files in the list
combined = lapply(trainingArticles,readerPlain)
# set article names
names(combined) = trainingArticles
names(combined) = sub('.txt','',names(combined))
# creates the corpus
trainCorpus = Corpus(VectorSource(combined))

```

After reading in the data, we pre-processed the text in the articles. 
* Converting all text to lowercase
* Remove numbers
* Remove punctuation
* Remove excess white space

```{r, echo=FALSE}
trainArticles = trainCorpus %>%
  tm_map(content_transformer(tolower))  %>%             # make everything lowercase
  tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
  tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
  tm_map(content_transformer(stripWhitespace))          # remove excess white-space
DTM_train = DocumentTermMatrix(trainArticles)
DTM_train
trainArticles = tm_map(trainArticles, content_transformer(removeWords), stopwords("en"))
DTM_train = DocumentTermMatrix(trainArticles)
DTM_train
DTM_train = removeSparseTerms(DTM_train, .99)
DTM_train
DTM_train = weightTfIdf(DTM_train)
DTM_train <- as.matrix(DTM_train)
```

After these four steps, we're down to **2500 documents** with **32,669 terms.** 
* Remove stop and filler words, based on the "basic English" stop words

After removing filler words, we're down to **32,570 terms.** 
* Removed words that have count 0 in > 99% of documents

Thus cuts the long tail significantly to only **3393 terms.**
* Finally, we converted the raw counts of words in each document to TF-IDF weights.

**Then, we replicated the same process to read in the 50 testing articles for the authors. There are 3448 terms in the testing data, compared to only 3393 terms in the training data. We will deal with this in the later procedure.**

```{r, echo=FALSE}
# expand all file paths in training data
test = Sys.glob('data/ReutersC50/C50test/*')
# initiate empty lists to be used in for loop
testingArticles = NULL
labels_test = NULL
# read in all training articles
for (name in test) {
  author = substring(name, first=20) # set author name
  article = Sys.glob(paste0(name,'/*.txt')) # expand articles for each name
  testingArticles = append(testingArticles,article) # append articles to list
  labels_test = append(labels_test, rep(author, length(article))) # append labels to list
}
# read all the plain text files in the list
combined = lapply(testingArticles,readerPlain)
# set article names
names(combined) = testingArticles
names(combined) = sub('.txt','',names(combined))
# creates the corpus
testCorpus = Corpus(VectorSource(combined))
```

For the testing data, we did the same pre-processing steps as the training data.

```{r, echo=FALSE}
testArticles = testCorpus %>%
  tm_map(content_transformer(tolower))  %>%             # make everything lowercase
  tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
  tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
  tm_map(content_transformer(stripWhitespace))          # remove excess white-space
DTM_test = DocumentTermMatrix(testArticles)
DTM_test
testArticles = tm_map(testArticles, content_transformer(removeWords), stopwords("en"))
DTM_test = DocumentTermMatrix(testArticles)
DTM_test
DTM_test = removeSparseTerms(DTM_test, .99)
DTM_test
DTM_test = weightTfIdf(DTM_test)
DTM_test <- as.matrix(DTM_test)
```

**We ignored words that are in the testing set and but not in the training set as below** 

```{r, echo=FALSE}
#forces test to take only identical col names as train
DTM_test = DocumentTermMatrix(testArticles, list(dictionary=colnames(DTM_train)))
DTM_test = weightTfIdf(DTM_test)
DTM_test
DTM_test <- as.matrix(DTM_test)
```

This removes the 55 "new" terms from the training data, less than 2% of the training terms. After this procedure, both of the training and testing groups have 3393 terms.

We now use PCA to simplify the predictors.
* We remove columns that have zero entries.
* We use only intersecting columns of the train and testing data.

```{r, echo=FALSE}
DTM_train <- DTM_train[,which(colSums(DTM_train) != 0)]
DTM_test <- DTM_test[,which(colSums(DTM_test) != 0)]
DTM_train = DTM_train[,intersect(colnames(DTM_test),colnames(DTM_train))]
DTM_test = DTM_test[,intersect(colnames(DTM_test),colnames(DTM_train))]
```

PCA process:

```{r, echo=FALSE}
pca = prcomp(DTM_train, scale =TRUE) #scale the data
predictions = predict(pca, newdata = DTM_test)
plot(cumsum(pca$sdev^2/sum(pca$sdev^2)), ylab = 'Cumulative variance explained', xlab = 'Number of principal components', main = 'Summary of Principal Component Variance Analysis')
#lets stop at 1000 principal components
#reformat the data
train = data.frame(pca$x[,1:1000])
train['author']=labels
train_load = pca$rotation[,1:1000]
test <- scale(DTM_test) %*% train_load
test <- as.data.frame(test)
test['author']=labels_test
```

We stop at 1000 principal components because it already can explain 80% of the variance as shown in the chart above.

We now can move on to the models. We chose to Naive Bayes and Random Forest to do so.

### Random Forest

The random forest model with mtry = =$\sqrt(n_{features})\approx 50$

```{r, echo=FALSE}
library(randomForest)
set.seed(1234)
mod_rand<-randomForest(as.factor(author)~.,data=train, mtry=50,importance=TRUE)
pre_rand<-predict(mod_rand,data=test)
tab_rand<-as.data.frame(table(pre_rand,as.factor(test$author)))
predicted<-pre_rand
actual<-as.factor(test$author)
temp<-as.data.frame(cbind(actual,predicted))
temp$flag<-ifelse(temp$actual==temp$predicted,1,0)
sum(temp$flag)/nrow(temp)
```


The Random Forest accuracy is 83.16%.

### Naive Bayes

We then used a Naive Bayes model to predict the testing data from a training data. 

```{r, echo=FALSE}
library('e1071')
mod_naive=naiveBayes(as.factor(author)~.,data=train)
pred_naive=predict(mod_naive,test)
library(caret)
predicted_nb=pred_naive
actual_nb=as.factor(test$author)
temp_nb<-as.data.frame(cbind(actual_nb,predicted_nb))
temp_nb$flag<-ifelse(temp_nb$actual_nb==temp_nb$predicted_nb,1,0)
sum(temp_nb$flag)/nrow(temp_nb)

```
The Naive Bayes accuracy is 96.8%, outperforming random forest.

**In summary, the Naive Bayes recieved the best accuracy of 96.8%, which is remarkably strong. .**

\newpage
## Association rule mining

```{r, include = FALSE}

file <- "https://raw.githubusercontent.com/jgscott/STA380/master/data/groceries.txt"
```
```{r}
library(reshape)
melt(iris)
library(tidyverse)
library(arules)  # has a big ecosystem of packages built around it
library(arulesViz)
df <- read.table(file, sep = ',', header = FALSE, fill = TRUE)
list(names(df))
# Preprocessing data

dfid <- tibble::rowid_to_column(df, "User")
df2 <- melt(dfid, id.vars = c("User"))
df2$variable <- NULL
attach(df2)
df2 <- df2[order(User),]
detach(df2)
df2 <- df2[!apply(df2 == "", 1, any),]
str(df2)
summary(df2)
# Barplot of top 20 items
# the dot (.) means "plug in the argument coming from the left"
summary(df2$value, maxsum=Inf)
#sort(df2$value, decreasing=TRUE)
#head(df2$value, 20)
frq = table(df2$value)
dffrq = as.data.frame(frq)
dffrq <- dffrq[-c(1),]
# sort
attach(dffrq)
dffrq <- dffrq[order(-Freq),]
barplot(dffrq$Freq[1:20], names=dffrq$Var1[1:20], las=2, cex.names=0.6)
detach(dffrq)
```




```{r, echo=FALSE}
# Turn user into a factor
df2$User = factor(df2$User)
# Split
grocs = split(x=df2$value, f=df2$User)
# Cast as "transactions"
grocstrans = as(grocs, "transactions")


grocrules = apriori(grocstrans, 
                     parameter=list(support=.005, confidence=.1, maxlen=4))


inspect(subset(grocrules, subset=lift > 2.0 & confidence > 0.2))

sub1 = subset(grocrules, subset=confidence > 0.2 & lift > 2)
plot(sub1, method='graph')
```

As a general heuristic, we chose a lift value of 2. We saw that lower lift values such as 1.5 gave some odd rules, such as onion therefore milk, or curd therefore other vegetables, and higher lift values gave perhaps less rules than one might desire, with 2.5 going down to 18 rules. In general, we also thought that in general doubling the likelihood of buying an item was a good baseline for if an item increased your likelihodo of purchasing. We still saw some odd rules at a lift of 2, such as sausage implies citrus fruit, and believe these to be since we have yet to adjust for confidence. Looking at the data, a confidence threshold of about 0.2 seemed to clean up these issues. It's also a good general value, as if the likelihood of something is less than 20\%, we generally consider it very unlikely. We stuck with general round values to prevent overtuning the parameters to get the exact data we wanted.  This leaves us with 17 rules.

These rule sets seem to make sense, with types of vegetables, fruit, and dairy implying types of vegetables, fruit and dairy, respectively. It also seems to be picking up on the existence of meals, with chicken, pork, and beef both implying vegetable purchases. Some rules do not entirely make sense, such as whole milk and yogurt implying other vegetables, although this could be picking up on the existence of families, who tend to buy milk and yogurt, but also want healthy children and therefore buy vegetables.




